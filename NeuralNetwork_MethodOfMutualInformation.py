# -*- coding: utf-8 -*-

import keras
import numpy as np
import os, time

from sklearn.preprocessing import minmax_scale
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif, chi2
from sklearn.model_selection import train_test_split
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.metrics import f1_score
from sklearn import metrics

from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential
from keras.layers import Dense

import pandas as pd
import matplotlib.pyplot as plt


def getParameters(line):

    parameters = line.split(',')

    for i in range(len(parameters)):
        parameters[i] = int(parameters[i])

    return parameters


def readDataset(filename):

    data = []

    with open(filename) as file:
        for line in file:
            parameters = getParameters(line)
            data.append(parameters)
        
    return data


def readLabels(filename):

    labels = []

    with open(filename) as file:
        for line in file:
            labels.append(int(line[0]))
        
    return labels


def myModel():

    # Создаем модель, где слои идут друг за другом
    model = Sequential()

    model.add(Dense(128, input_dim=10, activation='tanh'))
    model.add(Dense(64, activation='tanh'))
    
    # Выходной слой
    model.add(Dense(1, activation='sigmoid'))

    # Компилируем модель с нужными параметрами
    model.compile(loss='binary_crossentropy',
                  optimizer='adamax', # adam, sgd, rmsprop, adamax
                  metrics=['accuracy'])
    return model


def optimizeParameters(data, labels):

    # Выбираем k лучших признаков
    #######
    selecter = SelectKBest(mutual_info_classif, k=10)
    #######
    fit = selecter.fit(data, labels)
    
    """
    print('Selected tags')
    print(selecter.get_support())
    #"""

    return selecter.transform(data)


def getCountByValue(valuesList):
    value = valuesList[0]
    for i in range(1, len(valuesList)):
        if (valuesList[i] != value):
            return (i + 1)

    return len(valueList)


def separateDataset(dataset, labels, percentage):

    # Делим данные на обучение и тестирование (включая
    # нормальные и вредоносные файлы в одинаковом соотношении)

    datasetSize = len(dataset)

    firstDataSize = getCountByValue(labels)
    secondDataSize = datasetSize - firstDataSize

    # Части для обучения
    firstTrainPart = dataset[0:int(firstDataSize * percentage)]
    secondTrainPart = dataset[firstDataSize:
                              int(firstDataSize + secondDataSize * percentage)]

    # Части для тестирования
    firstTestPart = dataset[int(firstDataSize * percentage):firstDataSize]
    secondTestPart = dataset[int(firstDataSize + secondDataSize * percentage):
                             datasetSize]
    
    # Объединяем в единые np массивы
    x_train = np.vstack((firstTrainPart,secondTrainPart))
    y_train = np.vstack((firstTestPart, secondTestPart))

    return x_train, y_train


def separateLabels(labels, percentage):

    labelsSize = len(labels)

    firstLabelsSize = getCountByValue(labels)
    secondLabelsSize = labelsSize - firstLabelsSize

    firstTrainPart = labels[0:int(firstLabelsSize * percentage)]
    secondTrainPart = labels[firstLabelsSize:
                             int(firstLabelsSize + secondLabelsSize * percentage)]

    firstTestPart = labels[int(firstLabelsSize * percentage):firstLabelsSize]
    secondTestPart = labels[int(firstLabelsSize + secondLabelsSize * percentage):
                            labelsSize]
    
    x_test = firstTrainPart + secondTrainPart    
    y_test = firstTestPart + secondTestPart

    return x_test, y_test


def drawGraphic(dataset, labels):

    # Укажие сколько признаков пропустить, чтобы график получился более информативным
    offset = 0

    parametersCount = len(dataset[0]) - offset

    normalCountList = [0] * parametersCount
    malwareCountList = [0] * parametersCount 

    normalCount = getCountByValue(labels)
    malwareCount = len(dataset) - normalCount

    for i in range(0, len(dataset)):
        if(i < normalCount):
            for j in range(0, parametersCount):
                normalCountList[j] += dataset[i][j + offset]
        else:
            for j in range(0, parametersCount):
                malwareCountList[j] += dataset[i][j + offset]

    for i in range(0, parametersCount):
        normalCountList[i] = int(normalCountList[i] / normalCount)
        malwareCountList[i] = int(malwareCountList[i] / malwareCount)

    ts = pd.DataFrame({'normal': normalCountList,
                       'malware': malwareCountList})
    
    print('normal ', normalCountList)
    print('malware ', malwareCountList)
    ts.plot()
    plt.xlabel('Tag number\'s')
    plt.ylabel('Number of tag')
    plt.show()
    return


def drawROC(model, x, y):

    probs = model.predict_proba(x)
    fpr, tpr, threshold = metrics.roc_curve(y, probs)
    roc_auc = metrics.auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, lw = 2, label='Logistic Regression (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig('Log_ROC')
    plt.show()


def resultTesting(tpSum, tnSum, fpSum, fnSum, length, 
    fscore, blockCount,
    trainingTime, testingTime):
 
    print('[*] True positive:', tpSum / length) # Сколько определено нормальных файлов
    print('[*] True negative:', tnSum / length) # Сколько определено вредоносных файлов
    print('[*] False positive:', fpSum / length) # Ошибка 1-ого рода - сколько не пропустил нормальных
    print('[*] False negative:', fnSum / length) # Ошибка 2-ого рода - сколько пропушено вредоеносных 

    print('[*] F1-score:', fscore / blockCount) # Оценка качества
    print('[*] Precision:', tnSum / (tnSum + fnSum)) # Точность
    print('[*] Recall:', tnSum / (tnSum + tpSum)) # Полнота

    print('[*] Training time:', trainingTime)
    print('[*] Testing time:', testingTime)
    print()


def makeCrossValidation(dataset, labels, blockCount):

    print('[*] Cross validation (K = ' + str(blockCount) + ') has started')
    
    tnSum, fpSum, fnSum, tpSum = 0, 0, 0, 0
    trainingTime = 0.0
    testingTime = 0.0
    fscore = 0

    model = KerasClassifier(build_fn=myModel, epochs=10, verbose=0)
    kf = KFold(n_splits=blockCount, random_state=None, shuffle=False)
    
    for trainIndex, testIndex in kf.split(dataset):

        # Обучение
        begin = time.time()
        model.fit(dataset[trainIndex], labels[trainIndex])
        trainingTime += time.time() - begin

        # Тестирование
        begin = time.time()
        y_pred = model.predict(dataset[testIndex])
        testingTime += time.time() - begin

        tn, fp, fn, tp = confusion_matrix(labels[testIndex], y_pred, labels=[0, 1]).ravel()
        tnSum += tn
        fpSum += fp
        fnSum += fn
        tpSum += tp

        fscore += f1_score(labels[testIndex], y_pred, average='binary')

    resultTesting(tpSum, tnSum, fpSum, fnSum, len(dataset),
        fscore, blockCount,
        trainingTime, testingTime)


def makeRandomCrossValidation(dataset, labels):

    print('[*] Random cross validation has started')
    
    tn, fp, fn, fp = 0, 0, 0, 0
    trainingTime = 0.0
    testingTime = 0.0
    
    model = KerasClassifier(build_fn = myModel, epochs = 10, verbose = 0)
    x_train, x_test, y_train, y_test = train_test_split(dataset, labels,
                                                        test_size=0.33,
                                                        random_state=42)
    # Обучение 
    begin = time.time()
    model.fit(x_train, y_train)
    trainingTime = time.time() - begin
    
    # Тестирование
    begin = time.time()
    y_pred = model.predict(x_test)
    testingTime = time.time() - begin
	
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
    fscore = f1_score(y_test, y_pred, average='binary')

    resultTesting(tp, tn, fp, fn, len(y_test),
        fscore, 1,
        trainingTime, testingTime)


def makeItemCrossValidation(dataset, labels):

    print('[*] Item cross validation has started')
    
    tnSum, fpSum, fnSum, tpSum = 0, 0, 0, 0
    trainingTime = 0.0
    testingTime = 0.0
    fscore = 0

    model = KerasClassifier(build_fn=myModel, epochs=1, batch_size=10, verbose=0)

    loo = LeaveOneOut()
    loo.get_n_splits(dataset)
    
    for trainIndex, testIndex in loo.split(dataset):

        # Обучение
        begin = time.time()
        model.fit(dataset[trainIndex], labels[trainIndex])
        trainingTime += time.time() - begin

        # Тестирование
        begin = time.time()
        y_pred = model.predict(dataset[testIndex])
        testingTime += time.time() - begin

        tn, fp, fn, tp = confusion_matrix(labels[testIndex], y_pred, labels=[0, 1]).ravel()
        tnSum += tn
        fpSum += fp
        fnSum += fn
        tpSum += tp

        fscore += f1_score(labels[testIndex], y_pred, average='binary')

    resultTesting(tpSum, tnSum, fpSum, fnSum, len(dataset),
        fscore, len(dataset),
        trainingTime, testingTime)


def commonNeuralNetwork(dataset, labels):

    # Делим данные для обучения и тестирования в процентном соотношении
    x_train, x_test = separateDataset(dataset, labels, 0.9)
    y_train, y_test = separateLabels(labels, 0.9)

    model = myModel()

    # Обучаем сеть
    # x_train - данные для обучения
    # y_train - правильные ответы (0 - normal, 1 - malware)
    # batch_size - количество элементов из обучающей выборки,
    #              которое берется для обучения за один раз
    # epochs - количество повторов обучения
    # shuffle - перемешивать данные перед каждой эпохой
    model.fit(x_train, y_train, batch_size=4, epochs=10, shuffle=True)

    # Тестируем сеть
    score = model.evaluate(x_test, y_test, batch_size=4)
    print("[+] Model has been tested. Accuracy: %.2f%%" % (score[1] * 100))

    # Ошибки 1-го и 2-го рода
    pre_cls = model.predict_classes(x_train)
    confMatrix = confusion_matrix(y_train , pre_cls)
    print("[*] Confusion matrix: \n", confMatrix)

    # F-мера
    print("[*] F1-score:", f1_score(y_train, pre_cls, average='binary'))

    # Строим ROC кривую
    drawROC(model, x_train, y_train)
    
    
def main():
    
    import warnings
    warnings.filterwarnings('ignore')

    # Данные для сети 
    dataset = readDataset(os.getcwd() + "\\dataset\\parameters.csv")
    labels = readLabels(os.getcwd() + "\\dataset\\labels.csv")

    drawGraphic(dataset, labels)
    
    # Выбираем лучшие параметры
    dataset = optimizeParameters(dataset, labels)

    drawGraphic(dataset, labels)

    commonNeuralNetwork(dataset, labels)

    dataset = minmax_scale(dataset, feature_range=(0, 1), axis=0)
    dataset = np.array(dataset , "float32")
    labels = np.array(labels , "float32")

    # Перекрестная проверка
    makeCrossValidation(dataset, labels, 5)
    makeCrossValidation(dataset, labels, 10)
    makeCrossValidation(dataset, labels, 20)
    makeRandomCrossValidation(dataset, labels)
    makeItemCrossValidation(dataset, labels)
    
main()
