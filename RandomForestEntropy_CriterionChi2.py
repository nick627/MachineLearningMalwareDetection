# -*- coding: utf-8 -*-

import keras
import numpy as np
import os, time

from sklearn.preprocessing import minmax_scale
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.metrics import f1_score
from sklearn import metrics

#from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential
from keras.layers import Dense

import pandas as pd
import matplotlib.pyplot as plt

def getParameters(line):

    parameters = line.split(',')

    for i in range(len(parameters)):
        parameters[i] = int(parameters[i])

    return parameters

def readDataset(filename):
    data = []

    with open(filename) as file:
        for line in file:
            parameters = getParameters(line)
            data.append(parameters)

    return data

def readLabels(filename):
    labels = []

    with open(filename) as file:
        for line in file:
            labels.append(int(line[0]))

    return labels

def optimizeParameters(data, labels):
    # Выбираем k лучших признаков
    selecter = SelectKBest(chi2, k=10)
    selecter.fit(data, labels)
    return selecter.transform(data)

def getCountByValue(valuesList):
    value = valuesList[0]
    for i in range(1, len(valuesList)):
        if (valuesList[i] != value):
            return (i + 1)

    return len(valueList)

def separateDataset(dataset, labels, percentage):

    # Делим данные на обучение и тестирование (включая
    # нормальные и вредоносные файлы в одинаковом соотношении)

    datasetSize = len(dataset)

    firstDataSize = getCountByValue(labels)
    secondDataSize = datasetSize - firstDataSize

    # Части для обучения
    firstTrainPart = dataset[0:int(firstDataSize * percentage)]
    secondTrainPart = dataset[firstDataSize:
                              int(firstDataSize + secondDataSize * percentage)]

    # Части для тестирования
    firstTestPart = dataset[int(firstDataSize * percentage):firstDataSize]
    secondTestPart = dataset[int(firstDataSize + secondDataSize * percentage):
                             datasetSize]
    
    # Объединяем в единые np массивы
    x_train = np.vstack((firstTrainPart,secondTrainPart))
    y_train = np.vstack((firstTestPart, secondTestPart))

    return x_train, y_train

def separateLabels(labels, percentage):

    labelsSize = len(labels)

    firstLabelsSize = getCountByValue(labels)
    secondLabelsSize = labelsSize - firstLabelsSize

    firstTrainPart = labels[0:int(firstLabelsSize * percentage)]
    secondTrainPart = labels[firstLabelsSize:
                             int(firstLabelsSize + secondLabelsSize * percentage)]

    firstTestPart = labels[int(firstLabelsSize * percentage):firstLabelsSize]
    secondTestPart = labels[int(firstLabelsSize + secondLabelsSize * percentage):
                            labelsSize]
    
    x_test = firstTrainPart + secondTrainPart    
    y_test = firstTestPart + secondTestPart

    return x_test, y_test

def drawGraphic(dataset, labels):

    # Укажие сколько признаков пропустить, чтобы график получился более информативным
    offset = 0

    parametersCount = len(dataset[0]) - offset

    normalCountList = [0] * parametersCount
    malwareCountList = [0] * parametersCount 

    normalCount = getCountByValue(labels)
    malwareCount = len(dataset) - normalCount

    for i in range(0, len(dataset)):
        if(i < normalCount):
            for j in range(0, parametersCount):
                normalCountList[j] += dataset[i][j - offset]
        else:
            for j in range(0, parametersCount):
                malwareCountList[j] += dataset[i][j - offset]

    for i in range(0, parametersCount):
        normalCountList[i] = int(normalCountList[i] / normalCount)
        malwareCountList[i] = int(malwareCountList[i] / malwareCount)

    ts = pd.DataFrame({'normal': normalCountList,
                       'malware': malwareCountList})
    print('normal ', normalCountList)
    print('malware ', malwareCountList)
    ts.plot()
    plt.xlabel('Tag number\'s')
    plt.ylabel('Number of tag')
    plt.show()
    return

def drawROC(model, x, y):
    probs = model.predict(x)
    fpr, tpr, threshold = metrics.roc_curve(y, probs)
    roc_auc = metrics.auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, lw = 2, label='Logistic Regression (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig('Log_ROC')
    plt.show()


def resultTesting(tpSum, tnSum, fpSum, fnSum, length, 
    fscore, blockCount,
    trainingTime, testingTime):
 
    print('[*] True positive:', tpSum / length) # Сколько определено нормальных файлов
    print('[*] True negative:', tnSum / length) # Сколько определено вредоносных файлов
    print('[*] False positive:', fpSum / length) # Ошибка 1-ого рода - сколько не пропустил нормальных
    print('[*] False negative:', fnSum / length) # Ошибка 2-ого рода - сколько пропушено вредоеносных 

    print('[*] F1-score:', fscore / blockCount) # Оценка качества
    print('[*] Precision:', tnSum / (tnSum + fnSum)) # Точность
    print('[*] Recall:', tnSum / (tnSum + tpSum)) # Полнота

    print('[*] Training time:', trainingTime)
    print('[*] Testing time:', testingTime)
    print()


def makeCrossValidation(dataset, labels, blockCount):

    print('[*] Cross validation (K = ' + str(blockCount) + ') has started')
    
    tnSum, fpSum, fnSum, tpSum = 0, 0, 0, 0
    trainingTime = 0.0
    testingTime = 0.0
    fscore = 0

    #######
    #myClassifier = KNeighborsClassifier(n_neighbors=3)
    myClassifier = RandomForestClassifier(min_samples_leaf = 2, random_state=17,  criterion='entropy')
    #######
    kf = KFold(n_splits=blockCount, random_state=None, shuffle=False)
    
    for trainIndex, testIndex in kf.split(dataset):

        # Обучение
        begin = time.time()
        myClassifier.fit(dataset[trainIndex], labels[trainIndex])
        trainingTime += time.time() - begin

        # Тестирование
        begin = time.time()
        y_pred = myClassifier.predict(dataset[testIndex])
        testingTime += time.time() - begin

        tn, fp, fn, tp = confusion_matrix(labels[testIndex], y_pred, labels=[0, 1]).ravel()
        tnSum += tn
        fpSum += fp
        fnSum += fn
        tpSum += tp

        fscore += f1_score(labels[testIndex], y_pred, average='binary')

    resultTesting(tpSum, tnSum, fpSum, fnSum, len(dataset),
        fscore, blockCount,
        trainingTime, testingTime)


def makeRandomCrossValidation(dataset, labels):

    print('[*] Random cross validation has started')

    tn, fp, fn, fp = 0, 0, 0, 0
    trainingTime = 0.0
    testingTime = 0.0

    myClassifier = RandomForestClassifier(min_samples_leaf = 2, random_state=17,  criterion='entropy')
    x_train, x_test, y_train, y_test = train_test_split(dataset, labels,
                                                        test_size=0.33,
                                                        random_state=42)
    # Обучение 
    begin = time.time()
    myClassifier.fit(x_train, y_train)
    trainingTime = time.time() - begin

    # Тестирование
    begin = time.time()
    y_pred = myClassifier.predict(x_test)
    testingTime = time.time() - begin
	
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()
    fscore = f1_score(y_test, y_pred, average='binary')

    resultTesting(tp, tn, fp, fn, len(y_test),
        fscore, 1,
        trainingTime, testingTime)


def makeItemCrossValidation(dataset, labels):

    print('[*] Item cross validation has started')

    tnSum, fpSum, fnSum, tpSum = 0, 0, 0, 0
    trainingTime = 0.0
    testingTime = 0.0
    fscore = 0.0

    myClassifier = RandomForestClassifier(min_samples_leaf = 2, random_state=17,  criterion='entropy')

    loo = LeaveOneOut()
    loo.get_n_splits(dataset)
    LeaveOneOut()

    for trainIndex, testIndex in loo.split(dataset):

        # Обучение
        begin = time.time()
        myClassifier.fit(dataset[trainIndex], labels[trainIndex])
        trainingTime += time.time() - begin

        # Тестирование
        begin = time.time()
        y_pred = myClassifier.predict(dataset[testIndex])
        testingTime += time.time() - begin

        tn, fp, fn, tp = confusion_matrix(labels[testIndex], y_pred, labels=[0, 1]).ravel()
        tnSum += tn
        fpSum += fp
        fnSum += fn
        tpSum += tp

        fscore += f1_score(labels[testIndex], y_pred, average='binary')

    resultTesting(tpSum, tnSum, fpSum, fnSum, len(dataset),
        fscore, len(dataset),
        trainingTime, testingTime)


def commonKNeighborsClassifier(dataset, labels):

    # Делим данные для обучения и тестирования в процентном соотношении
    x_train, x_test = separateDataset(dataset, labels, 0.9)
    y_train, y_test = separateLabels(labels, 0.9)

    myClassifier = RandomForestClassifier(min_samples_leaf = 2, random_state=17,  criterion='entropy')

    # Обучаем сеть
    myClassifier.fit(x_train, y_train)

    # Тестируем сеть
    score = myClassifier.score(x_test, y_test)
    print("[+] Model has been tested. Accuracy: %.2f%%" % (score * 100))

    # Ошибки 1-го и 2-го рода
    pre_cls = myClassifier.predict(x_train)
    confMatrix = confusion_matrix(y_train , pre_cls)
    print("[*] Confusion matrix: \n", confMatrix)

    # F-мера
    print("[*] F1-score:", f1_score(y_train, pre_cls, average='binary'))

    # Строим ROC кривую
    drawROC(myClassifier, x_train, y_train)

def main():

    import warnings
    warnings.filterwarnings('ignore')

    # Данные для сети 
    dataset = readDataset(os.getcwd() + "\\dataset\\parameters.csv")
    labels = readLabels(os.getcwd() + "\\dataset\\labels.csv")

    drawGraphic(dataset, labels)

    # Выбираем лучшие параметры
    dataset = optimizeParameters(dataset, labels)

    drawGraphic(dataset, labels)

    commonKNeighborsClassifier(dataset, labels)

    dataset = minmax_scale(dataset, feature_range=(0, 1), axis = 0)
    dataset = np.array(dataset , "float32")
    labels = np.array(labels , "float32")

    # Перекрестная проверка
    makeCrossValidation(dataset, labels, 5)
    makeCrossValidation(dataset, labels, 10)
    makeCrossValidation(dataset, labels, 20)
    makeRandomCrossValidation(dataset, labels)
    makeItemCrossValidation(dataset, labels)

main()
